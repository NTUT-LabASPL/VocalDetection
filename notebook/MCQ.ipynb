{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "# PATH TO PROJECT ROOT\n",
    "sys.path.append(\"/path/to/VocalDetection\")\n",
    "from definitions import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(cls_results, ground_truth):\n",
    "    acc = []\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    total = ground_truth.shape[0]\n",
    "    for v in np.array(cls_results):\n",
    "        acc.append(np.sum(ground_truth[:,0] == v[:,0])/total)\n",
    "\n",
    "    return np.sum(acc)/len(acc) \n",
    "\n",
    "def voting(cls_results, ground_truth, title=\"Default\"):\n",
    "    \"\"\"\n",
    "    To calculate voting accuracy.\n",
    "\n",
    "        cls_results == np.where(cls.predict(test_ds) >= 0.5, 1, 0)\n",
    "        ground_truth == [ y.numpy() for _, y in dataset.load(test_ds_path)]\n",
    "        title(str)\n",
    "    \"\"\"\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    voting_0_count = np.zeros(ground_truth.shape[0])\n",
    "    voting_1_count = np.zeros(ground_truth.shape[0])\n",
    "    cls_results = np.array(cls_results)\n",
    "    for v in cls_results:\n",
    "        for i, value in enumerate(v):\n",
    "            if value[0] == 1:\n",
    "                voting_1_count[i] += 1\n",
    "            else:\n",
    "                voting_0_count[i] += 1\n",
    "\n",
    "    voting_result_rates = [0 for i in range(cls_results.shape[0]//2 + 1)] # e.g. 7 = 0 1 2 3   21 = 0 1 2 ... 10\n",
    "    voting_result = np.zeros(ground_truth.shape[0])\n",
    "\n",
    "    for i in range(ground_truth.shape[0]):\n",
    "        x, y = voting_0_count[i], voting_1_count[i]\n",
    "        if x < y:\n",
    "            voting_result[i] = 1\n",
    "        voting_result_rates[int(min(x, y))] += 1\n",
    "    return np.sum(voting_result == ground_truth[:, 0]) / ground_truth.shape[0], voting_result_rates\n",
    "\n",
    "def classfication_voting(filename, train_list, test_list, tops=4, nums=None, axs=None, df=None, test_df=None, title='Default', debug=False):\n",
    "    _cls_results = json.load(open(filename))\n",
    "    _keys = list(_cls_results['ground_truth'].keys())\n",
    "\n",
    "    total_classfication = np.asarray(_cls_results[_keys[0]]).shape[0]\n",
    "\n",
    "    if isinstance(nums, int):\n",
    "        if nums > total_classfication:\n",
    "            nums = total_classfication + 1\n",
    "        elif nums <= 0:\n",
    "            nums = total_classfication + 1\n",
    "    else:\n",
    "        nums = total_classfication + 1\n",
    "\n",
    "    _train_keys = [ key for key in _keys if key in train_list]\n",
    "    _test_keys = [ key for key in _keys if key in test_list]\n",
    "    if debug:\n",
    "        print('train_keys: ' + ', '.join(_train_keys).replace('SCNN-', '').replace('.h5', ''))\n",
    "        print('test_keys: ' + ', '.join(_test_keys).replace('SCNN-', '').replace('.h5', ''))\n",
    "\n",
    "    _ground_truth = [ np.asarray(x) for x in _cls_results['ground_truth'].values()]\n",
    "    _cls_predict = [ np.asarray(list(x)[:nums]) for x in _cls_results.values()][:-1] # last one is ground_truth\n",
    "    x = []   # Average accuracies\n",
    "    y = []   # Voting accuracies\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "\n",
    "    top_y = [[] for i in range(tops)] # Top n high Classification confidence\n",
    "    test_top_y = [[] for i in range(tops)]\n",
    "\n",
    "    for i, key in enumerate(_keys):\n",
    "        if key not in _train_keys and key not in _test_keys:\n",
    "            continue\n",
    "\n",
    "        total = _ground_truth[i].shape[0]\n",
    "        acc, acc_rates = voting(_cls_predict[i], _ground_truth[i])\n",
    "        print(_cls_predict[i].shape)\n",
    "        exit()\n",
    "        avg_acc = avg(_cls_predict[i], _ground_truth[i])\n",
    "        if key in _test_keys:\n",
    "            test_x.append(avg_acc)\n",
    "            test_y.append(acc)\n",
    "            for j in range(tops):\n",
    "                test_top_y[j].append(np.sum(acc_rates[:j+1])/total)\n",
    "        elif key in _train_keys:\n",
    "            x.append(avg_acc)\n",
    "            y.append(acc)\n",
    "            for j in range(tops):\n",
    "                top_y[j].append(np.sum(acc_rates[:j+1])/total)\n",
    "\n",
    "    x = np.array(x).reshape(-1, 1)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    test_x = np.array(test_x).reshape(-1, 1)\n",
    "    test_y = np.array(test_y).reshape(-1, 1)\n",
    "    test_top_y = np.array(test_top_y).reshape(tops, -1, 1)\n",
    "    top_y = np.array(top_y).reshape(tops, -1, 1)\n",
    "    \n",
    "\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(nrows=math.ceil(tops/2), ncols=2, figsize=(18, 4*math.ceil(tops/2)))\n",
    "        fig.suptitle(title)\n",
    "\n",
    "    for i, _y in enumerate(top_y):\n",
    "        lr = LinearRegression().fit(_y, x)\n",
    "        score = lr.score(_y, x)\n",
    "\n",
    "        avg_diffs = []\n",
    "        avg_diffs_test = []\n",
    "        for _i, _k in enumerate(_train_keys):\n",
    "            origin_acc = x[_i][0]\n",
    "            predict_acc = lr.predict([[top_y[i][_i][0]]])[0][0]\n",
    "            avg_diffs.append(abs(origin_acc - predict_acc))\n",
    "        for _i, _k in enumerate(_test_keys):\n",
    "            origin_acc = test_x[_i][0]\n",
    "            predict_acc = lr.predict([[test_top_y[i][_i][0]]])[0][0]\n",
    "            avg_diffs_test.append(abs(origin_acc - predict_acc))\n",
    "            # print(_i, _k)\n",
    "            print( _k, origin_acc, predict_acc, abs(origin_acc - predict_acc), test_top_y[i][_i][0])\n",
    "\n",
    "        diff = sum(avg_diffs)/len(avg_diffs)\n",
    "        axs[int(i/2)][i%2].scatter(nums, diff*100)\n",
    "        df[i][0].append(nums)\n",
    "        df[i][1].append(diff*100)\n",
    "        test_diff = sum(avg_diffs_test)/len(avg_diffs_test)\n",
    "        axs[int(i/2)][i%2].scatter(nums, test_diff*100)\n",
    "        test_df[i][0].append(nums)\n",
    "        test_df[i][1].append(test_diff*100)\n",
    "    # plt.show()\n",
    "    # plt.clf()\n",
    "    return df, test_df, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_error(cls_result_path, train_list, test_list, title, cls_max, cls_min, step, tops=4, debug=False, save=False):\n",
    "    df = [[[], []] for _ in range(tops)]\n",
    "    test_df = [[[], []] for _ in range(tops)]\n",
    "    axs = None\n",
    "\n",
    "    for i in range(cls_min, cls_max+1, step):\n",
    "        df, test_df, axs = classfication_voting(cls_result_path, train_list=train_list, test_list=test_list, tops=4, df=df, test_df=test_df, axs=axs, nums=i, title=title, debug=debug)\n",
    "\n",
    "    for i in range(tops):\n",
    "        axs[int(i/2)][i%2].plot(df[i][0], df[i][1], label='Train error')\n",
    "        axs[int(i/2)][i%2].plot(test_df[i][0], test_df[i][1], label='Test error')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i in range(tops):\n",
    "        axs[0].plot(df[i][0][:], df[i][1][:], label=f'Top {i+1} Train')\n",
    "\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylabel('differences')\n",
    "    axs[0].set_xlabel('Classfications')\n",
    "\n",
    "    for i in range(tops):\n",
    "        axs[1].plot(test_df[i][0][:], test_df[i][1][:], label=f'Top {i+1} Test')\n",
    "    axs[1].legend()\n",
    "    axs[1].set_ylabel('differences %')\n",
    "    axs[1].set_xlabel('Classfications')\n",
    "    if save:\n",
    "        plt.savefig(title + '.png', bbox_inches='tight', facecolor='w')\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classification_top_value(cls_result_path, train_list, test_list, tops=4, title='Default', debug=False, save=False):\n",
    "    df = [[[], []] for _ in range(tops)]\n",
    "    test_df = [[[], []] for _ in range(tops)]\n",
    "    df, test_df, axs = classfication_voting(cls_result_path, train_list=train_list, test_list=test_list, tops=tops, df=df, test_df=test_df, title=title, debug=debug)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    x = []\n",
    "    y = []\n",
    "    test_y = []\n",
    "    for i, _ in enumerate(df):\n",
    "        x.append(i+1)\n",
    "        y.append(df[i][1])\n",
    "        test_y.append(test_df[i][1])\n",
    "    \n",
    "    plt.plot(x, y, label=f'Train error')\n",
    "    plt.plot(x, test_y, label=f'Test error')\n",
    "    plt.legend()\n",
    "    plt.ylabel('differences %')\n",
    "    plt.xlabel('top')\n",
    "    plt.title(title)\n",
    "    if save:\n",
    "        plt.savefig(title + '.png', bbox_inches='tight', facecolor='w')\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()\n",
    "    return y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def leave_one_out_test(cls_result_path, dataset_list, tops=4, title='default', cls_tops=4, cls_max=21, cls_min=9, step=2, debug=False, save=False):\n",
    "    _cls_results = json.load(open(cls_result_path))\n",
    "    _keys = list(_cls_results['ground_truth'].keys())\n",
    "    for dataset in dataset_list:\n",
    "        if dataset not in _keys:\n",
    "            raise ValueError(\"Dataset didn't exist!\")\n",
    "    all_y = []\n",
    "    all_test_y = []\n",
    "\n",
    "    cls_df = []\n",
    "    cls_test_df = []\n",
    "    dataset_list = np.asarray(dataset_list, dtype=str)\n",
    "    loo = LeaveOneOut()\n",
    "    print(\"Total split(s): \", loo.get_n_splits(dataset_list))\n",
    "    \n",
    "    for train_index, test_index in loo.split(dataset_list):\n",
    "        print(\"TRAIN: \", train_index, \"TEST: \", test_index)\n",
    "        # Top \n",
    "        y, test_y = compare_classification_top_value(cls_result_path, dataset_list[train_index], dataset_list[test_index], tops=tops, title='A_' + dataset_list[test_index[0]], debug=debug, save=save)\n",
    "        all_y.append(y)\n",
    "        all_test_y.append(test_y)\n",
    "        # Classfication\n",
    "        df, test_df = get_train_test_error(cls_result_path, dataset_list[train_index], dataset_list[test_index], title='B_' + dataset_list[test_index[0]], cls_max=cls_max, cls_min=cls_min, step=step, tops=cls_tops, debug=debug, save=save)\n",
    "        cls_df.append(df)\n",
    "        cls_test_df.append(test_df)\n",
    "\n",
    "    # Classfication\n",
    "\n",
    "    cls_df = np.array(cls_df).sum(axis=0) / len(dataset_list)\n",
    "    cls_test_df = np.array(cls_test_df).sum(axis=0) / len(dataset_list)\n",
    "\n",
    "    plt.clf()\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i in range(cls_tops):\n",
    "        axs[0].plot(cls_df[i][0][:], cls_df[i][1][:], label=f'Top {i+1} Train')\n",
    "\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylabel('differences')\n",
    "    axs[0].set_xlabel('Classfications')\n",
    "\n",
    "    for i in range(cls_tops):\n",
    "        axs[1].plot(cls_test_df[i][0][:], cls_test_df[i][1][:], label=f'Top {i+1} Test')\n",
    "    axs[1].legend()\n",
    "    axs[1].set_ylabel('differences %')\n",
    "    axs[1].set_xlabel('Classfications')\n",
    "    if save:\n",
    "        plt.savefig('Average_cls.png', bbox_inches='tight', facecolor='w')\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    # Top\n",
    "    avg_y = np.array(all_y).sum(axis=0) / len(dataset_list)\n",
    "    avg_test_y = np.array(all_test_y).sum(axis=0) / len(dataset_list)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    x = range(1, tops + 1)\n",
    "    plt.plot(x, avg_y, label=f'Train error')\n",
    "    plt.plot(x, avg_test_y, label=f'Test error')\n",
    "    print(\"X: \", x)\n",
    "    print(\"AVG_Y:\", avg_y)\n",
    "    print(\"AVG_TEST_Y:\", avg_test_y)\n",
    "    plt.legend()\n",
    "    plt.ylabel('differences %')\n",
    "    plt.xlabel('top')\n",
    "    plt.title(title + '_Average')\n",
    "    if save:\n",
    "        plt.savefig('Average.png', bbox_inches='tight', facecolor='w')\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_test(os.path.join(DATA_DIR, 'final/MCQ/sync_FMA-C-1_2021-02-13_11_SCNN18_SCNN-FMA-C-1-fixed-train_h5_2GPU_cls_result.json'),\n",
    "    dataset_list=[\n",
    "        # ---- Test\n",
    "        'SCNN-Jamendo-test.h5',\n",
    "        'SCNN-FMA-C-1-fixed-test.h5',\n",
    "        'SCNN-FMA-C-2-fixed-test.h5',\n",
    "        'SCNN-KTV-test.h5',\n",
    "        'SCNN-Taiwanese-CD-test.h5',\n",
    "        'SCNN-Taiwanese-stream-test.h5',\n",
    "        'SCNN-Chinese-CD-test.h5',\n",
    "        'SCNN-Classical-test.h5',\n",
    "        # ---- Only have one\n",
    "        'SCNN-MIR-1k-train.h5',\n",
    "        'SCNN-Instrumental-non-vocal.h5',\n",
    "        'SCNN-A-Cappella-vocal.h5',\n",
    "        'SCNN-test-hard.h5',\n",
    "    ],\n",
    "    tops = 4,\n",
    "    title='Default',\n",
    "    debug=True,\n",
    "    save=False,\n",
    "    cls_min=21,\n",
    "    cls_max=21,\n",
    "    cls_tops=4,\n",
    "    step=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_test_error('final/MCQ/sync_FMA-C-1_2021-02-13_11_SCNN18_SCNN-FMA-C-1-fixed-train_h5_2GPU_cls_result.json', [\n",
    "        # ---- Test\n",
    "        'SCNN-Jamendo-test.h5',\n",
    "        'SCNN-FMA-C-1-fixed-test.h5',\n",
    "        'SCNN-FMA-C-2-fixed-test.h5',\n",
    "        'SCNN-KTV-test.h5',\n",
    "        'SCNN-Taiwanese-CD-test.h5',\n",
    "        'SCNN-Taiwanese-stream-test.h5',\n",
    "        'SCNN-Chinese-CD-test.h5',\n",
    "        'SCNN-Classical-test.h5',\n",
    "        # ---- Only have one\n",
    "        'SCNN-MIR-1k-train.h5',\n",
    "        'SCNN-Instrumental-non-vocal.h5',\n",
    "        'SCNN-A-Cappella-vocal.h5',\n",
    "        'SCNN-test-hard.h5',\n",
    "    ], ['SCNN-RWC.h5'], cls_max=21, cls_min=21, step=2, tops=4, debug=True, save=False, title='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_test_error('final/MCQ/sync_FMA-C-1_2021-02-13_11_SCNN18_SCNN-FMA-C-1-fixed-train_h5_2GPU_cls_result.json', [\n",
    "        # ---- Test\n",
    "        'SCNN-Jamendo-test.h5',\n",
    "        'SCNN-FMA-C-1-fixed-test.h5',\n",
    "        'SCNN-FMA-C-2-fixed-test.h5',\n",
    "        'SCNN-KTV-test.h5',\n",
    "        # 'SCNN-Taiwanese-CD-test.h5',\n",
    "        # 'SCNN-Taiwanese-stream-test.h5',\n",
    "        # 'SCNN-Chinese-CD-test.h5',\n",
    "        # 'SCNN-Classical-test.h5',\n",
    "        # ---- Only have one\n",
    "        'SCNN-MIR-1k-train.h5',\n",
    "        'SCNN-Instrumental-non-vocal.h5',\n",
    "        'SCNN-A-Cappella-vocal.h5',\n",
    "        'SCNN-test-hard.h5',\n",
    "    ], [\n",
    "        # # ---- Test\n",
    "        # 'SCNN-Jamendo-test.h5',\n",
    "        # 'SCNN-FMA-C-1-fixed-test.h5',\n",
    "        # 'SCNN-FMA-C-2-fixed-test.h5',\n",
    "        # 'SCNN-KTV-test.h5',\n",
    "        'SCNN-Taiwanese-CD-test.h5',\n",
    "        'SCNN-Taiwanese-stream-test.h5',\n",
    "        'SCNN-Chinese-CD-test.h5',\n",
    "        'SCNN-Classical-test.h5',\n",
    "        # ---- Only have one\n",
    "        # 'SCNN-MIR-1k-train.h5',\n",
    "        # 'SCNN-Instrumental-non-vocal.h5',\n",
    "        # 'SCNN-A-Cappella-vocal.h5',\n",
    "        # 'SCNN-test-hard.h5',\n",
    "    ], cls_max=21, cls_min=21, step=2, tops=4, debug=True, save=False, title='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jamendo_acc_rtq = [\n",
    "0.9402880192,\n",
    "0.8255813897,\n",
    "0.8579096556,\n",
    "0.9403072417,\n",
    "0.8897533298,\n",
    "0.8260428429,\n",
    "0.9105074108,\n",
    "0.8206219256,\n",
    "0.8696308136,\n",
    "0.8019617081,\n",
    "0.9476521134,\n",
    "0.6601857483,\n",
    "]\n",
    "\n",
    "jamendo_rtq = [\n",
    "0.8134461343,\n",
    "0.9082952678,\n",
    "0.9066032767,\n",
    "0.6797484457,\n",
    "0.6485662937,\n",
    "0.8276259661,\n",
    "0.8641012132,\n",
    "0.6629108369,\n",
    "0.8367223144,\n",
    "0.6772853196,\n",
    "0.8127648413,\n",
    "0.9150333107,\n",
    "]\n",
    "\n",
    "jamendo_acc_mcq = [\n",
    "    0.9375498357,\n",
    "0.8239435799,\n",
    "0.8572675415,\n",
    "0.9401539694,\n",
    "0.8878648233,\n",
    "0.8261663177,\n",
    "0.9100006415,\n",
    "0.81470657,\n",
    "0.8702013287,\n",
    "0.7971706573,\n",
    "0.9501869417,\n",
    "0.6614286289,\n",
    "]\n",
    "\n",
    "\n",
    "jamendo_mcq = [\n",
    "0.9082384461,\n",
    "0.805630355,\n",
    "0.8424612699,\n",
    "0.9056327725,\n",
    "0.8785578748,\n",
    "0.821307779,\n",
    "0.9101930849,\n",
    "0.8191489362,\n",
    "0.8226837061,\n",
    "0.8436911488,\n",
    "0.9324665489,\n",
    "0.6819442769,\n",
    "]\n",
    "\n",
    "jamendo_acc_bzq = [\n",
    "0.9402880192,\n",
    "0.8255813897,\n",
    "0.8579096556,\n",
    "0.9403072417,\n",
    "0.8897533298,\n",
    "0.8260428429,\n",
    "0.9105074108,\n",
    "0.8206219256,\n",
    "0.8696308136,\n",
    "0.8019617081,\n",
    "0.9476521134,\n",
    "0.6601857483,\n",
    "]\n",
    "\n",
    "jamendo_bzq = [\n",
    "0.133088,\n",
    "0.221322,\n",
    "0.192189,\n",
    "0.089027,\n",
    "0.15351,\n",
    "0.173506,\n",
    "0.120251,\n",
    "0.16117,\n",
    "0.191427,\n",
    "0.255446,\n",
    "0.077291,\n",
    "0.313183,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(jamendo_acc_rtq, jamendo_rtq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmac1_acc_rtq = [\n",
    "0.928466171,\n",
    "0.8968665957,\n",
    "0.9080078483,\n",
    "0.9568397999,\n",
    "0.9383301735,\n",
    "0.8878241241,\n",
    "0.9299506009,\n",
    "0.8950491011,\n",
    "0.8933262348,\n",
    "0.8710138142,\n",
    "0.942426151,\n",
    "0.7259317398,\n",
    "]\n",
    "\n",
    "fmac1_rtq = [\n",
    "0.7009548008,\n",
    "0.86113922,\n",
    "0.8570181191,\n",
    "0.5442712516,\n",
    "0.525697735,\n",
    "0.6910967827,\n",
    "0.7558185041,\n",
    "0.6676432133,\n",
    "0.7884119511,\n",
    "0.6505630851,\n",
    "0.7013464987,\n",
    "0.8453566194,\n",
    "]\n",
    "\n",
    "fmac1_acc_mcq = [\n",
    "0.9273434759,\n",
    "0.8962872297,\n",
    "0.906954272,\n",
    "0.956143101,\n",
    "0.9345802837,\n",
    "0.8868577871,\n",
    "0.9284110591,\n",
    "0.8934416647,\n",
    "0.8916611728,\n",
    "0.8719023705,\n",
    "0.9433103714,\n",
    "0.7243884648,\n",
    "]\n",
    "\n",
    "\n",
    "fmac1_mcq = [\n",
    "0.8901540522,\n",
    "0.8641370869,\n",
    "0.8876281911,\n",
    "0.9312362838,\n",
    "0.8918406072,\n",
    "0.8675310034,\n",
    "0.9164795689,\n",
    "0.8711129296,\n",
    "0.8548100816,\n",
    "0.8769617075,\n",
    "0.9248927039,\n",
    "0.7136654203,\n",
    "]\n",
    "\n",
    "fmac1_acc_bzq = [\n",
    "0.928466171,\n",
    "0.8968665957,\n",
    "0.9080078483,\n",
    "0.9568397999,\n",
    "0.9383301735,\n",
    "0.8878241241,\n",
    "0.9299506009,\n",
    "0.8950491011,\n",
    "0.8933262348,\n",
    "0.8710138142,\n",
    "0.942426151,\n",
    "0.7259317398,\n",
    "]\n",
    "\n",
    "fmac1_bzq = [\n",
    "0.121735,\n",
    "0.144088,\n",
    "0.124395,\n",
    "0.069788,\n",
    "0.10019,\n",
    "0.135457,\n",
    "0.089044,\n",
    "0.123241,\n",
    "0.140327,\n",
    "0.153311,\n",
    "0.0859,\n",
    "0.255084,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(fmac1_acc_mcq, fmac1_mcq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "\n",
    "lr.fit(np.array(jamendo_mcq).reshape(-1, 1), np.array(jamendo_acc_mcq).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "\n",
    "lr.fit(np.array(jamendo_mcq).reshape(-1, 1), np.array(jamendo_acc_mcq).reshape(-1, 1))\n",
    "score = lr.score(np.array(jamendo_mcq).reshape(-1, 1), np.array(jamendo_acc_mcq).reshape(-1, 1))\n",
    "plt.title(f'MCQ Top 4 \\nscore: {score:.4f}')\n",
    "plt.scatter(jamendo_acc_mcq, jamendo_mcq)\n",
    "line = np.arange(0, 1, 0.001).reshape(-1, 1)\n",
    "plt.plot(lr.predict(line), line, linewidth=1)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('MCQ Top 4')\n",
    "plt.axis([min(jamendo_acc_mcq)-0.05, max(jamendo_acc_mcq)+0.05, min(jamendo_mcq)-0.05, max(jamendo_mcq)+0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "\n",
    "lr.fit(np.array(fmac1_rtq).reshape(-1, 1), np.array(fmac1_acc_rtq).reshape(-1, 1))\n",
    "score = lr.score(np.array(fmac1_rtq).reshape(-1, 1), np.array(fmac1_acc_rtq).reshape(-1, 1))\n",
    "plt.title(f'FMA-C-1 RTQ \\nscore: {score:.4f}')\n",
    "plt.scatter(fmac1_acc_rtq, fmac1_rtq)\n",
    "line = np.arange(0, 1, 0.001).reshape(-1, 1)\n",
    "plt.plot(lr.predict(line), line, linewidth=1)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('FMA-C-1 RTQ')\n",
    "plt.axis([min(fmac1_acc_rtq)-0.05, max(fmac1_acc_rtq)+0.05, min(fmac1_rtq)-0.05, max(fmac1_rtq)+0.05])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('usr')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
